{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Economic Data Processing and Analysis\n",
    "\n",
    "## Overview\n",
    "This notebook processes economic data from multiple Excel files, handles missing data, and prepares the dataset for analysis. The data includes various economic indicators such as GDP, debt ratios, inflation, and unemployment rates.\n",
    "\n",
    "## Data Sources\n",
    "- **IMF Data**: GDP, inflation, unemployment, government debt metrics\n",
    "- **BIS Data**: Debt service ratios for households and corporations\n",
    "\n",
    "## Processing Steps\n",
    "1. [Environment Setup - Data Loading](#env-setup)  \n",
    "2. [Data Cleaning - Missing Values](#data-cleaning)   \n",
    "3. [Time Period Analysis](#time-analysis)\n",
    "4. [Country Selection Analysis](#country-analysis) \n",
    "5. [Data Imputation and Final Cleaning](#data-imputation) \n",
    "6. [BIS Data Integration](#bis-integration) \n",
    "7. [Data Format Transformation](#data-format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"env-setup\"></a>\n",
    "## 1. Environment Setup - Data Loading\n",
    "### Required Libraries\n",
    "Install the necessary libraries before running this notebook:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "pip install pandas numpy matplotlib openpyxl\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set display options for better readability\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Discovery and Filtering\n",
    "We load all Excel files from the Data folder, excluding BIS debt service ratio files which require special handling due to their different structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define files to handle separately (BIS data has different structure)\n",
    "ignore_list=['Data\\\\Debt service ratios - Households (BIS).xlsx','Data\\\\Debt service ratios - Nonfinancial corporate (BIS).xlsx']\n",
    "# Get all Excel files from Data folder\n",
    "files = glob.glob(\"Data/*.xlsx\") \n",
    "files.remove(ignore_list[0])   \n",
    "files.remove(ignore_list[1])\n",
    "print(files)\n",
    "print('Number of files in Data folder: '+str(len(files)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data into DataFrames\n",
    "Each Excel file is loaded into a separate DataFrame and stored in a list for batch processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize list to store DataFrames\n",
    "df_list=[]\n",
    "for i,file in enumerate(files,1):\n",
    "    # Extract clean filename\n",
    "    name=file.split('\\\\')[-1]\n",
    "    name=name.replace('.xlsx', '') \n",
    "    print(f'Loading {i}/{len(files)}: {name}')\n",
    "    \n",
    "    df_name = pd.read_excel(file, engine=\"openpyxl\")\n",
    "    df_list.append(df_name)\n",
    "    # Display basic info about the dataset\n",
    "\n",
    "    print(f\"   Shape: {df_name.shape}\")\n",
    "    print(f\"   Columns: {list(df_name.columns[:5])}{'...' if len(df_name.columns) > 5 else ''}\")\n",
    "    print('    Sample data:' )\n",
    "    print(df_name.head(2).to_string())\n",
    "    print(\"- \" * 80)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total data points\n",
    "total_rows = sum(len(df) for df in df_list)\n",
    "print(\"Total data points (rows):\", total_rows)\n",
    "\n",
    "total_cells = sum(df.size for df in df_list)\n",
    "print(\"Total data points (cells):\", total_cells)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id=\"data-cleaning\"></a>\n",
    "## 2. Data Cleaning - Missing Values\n",
    "### Standardizing Missing Data Representation\n",
    "Convert 'no data' strings to NaN for consistent handling of missing values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, df in enumerate(df_list):\n",
    "    df_list[i] = df.replace('no data', np.nan,regex=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Missing Data Patterns\n",
    "Calculate the percentage of missing data for each variable to understand data quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_list=[]\n",
    "\n",
    "for i,df_metric in enumerate(df_list, 1):\n",
    "    missing_pct = df_metric.isnull().mean() * 100\n",
    "    \n",
    "    missing_list.append(missing_pct)\n",
    "    print(f\"Sample Data: \\n --{missing_list[-1].head(4)}\")\n",
    "    print(f\"Total missing values: {df_metric.isnull().sum().sum()}\")\n",
    "    print(f\"Average missing percentage: {missing_pct.mean():.2f}%\")\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"time-analysis\"></a>\n",
    "## 3. Time Period Analysis\n",
    "\n",
    "### Creating Missing Data by Year Matrix\n",
    "We create a comprehensive view of missing data patterns across years to determine the optimal time period for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract column names (metric names) for the combined DataFrame\n",
    "Col_names=[]\n",
    "for i in range(0,len(missing_list)):\n",
    "    Col_names.append(missing_list[i].index[0])\n",
    "\n",
    "print(Col_names)\n",
    "\n",
    "# Create combined DataFrame with missing data percentages by year\n",
    "df_miss_year=pd.concat(missing_list,axis=1,keys= [f\"{i}\" for i in Col_names])\n",
    "#Sample of missing data by year\n",
    "print(df_miss_year.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Summary of Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_miss_year.describe())\n",
    "print(df_miss_year.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Years in Analysis\n",
    "Fill NaN values with 100% to represent complete absence of data for those years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_miss_year.fillna(100,inplace=True)\n",
    "df_miss_year.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Data for Time Period Visualization\n",
    "\n",
    "Filter and prepare data for plotting to identify optimal time periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_miss_year.fillna(100,inplace=True)\n",
    "df_miss_year.index = df_miss_year.index.astype(str)\n",
    "\n",
    "# Filter rows where index represents a valid 4-digit year\n",
    "valid_years = df_miss_year.index[df_miss_year.index.str.match(r'^\\d{4}$')]\n",
    "df_m_plot = df_miss_year.loc[valid_years].copy()\n",
    "df_m_plot.index = df_m_plot.index.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Missing Data Trends by Year\n",
    "\n",
    "Create a comprehensive visualization to identify the optimal time period for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average missing data percentage per year\n",
    "average_missing_per_year = df_m_plot.mean(axis=1)\n",
    "average_missing_per_year = average_missing_per_year.sort_index()\n",
    "\n",
    "# Create decade markers for x-axis\n",
    "decade_years = [year for year in average_missing_per_year.index if year % 10 == 0]\n",
    "# Define target period and calculate its average\n",
    "y_value=average_missing_per_year.loc[1990:2023].mean()\n",
    "\n",
    "# Create the visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(average_missing_per_year.index, average_missing_per_year.values, color='mediumseagreen')\n",
    "\n",
    "# Highlight target period\n",
    "plt.plot([1990,2023],[y_value,y_value],color='red', linestyle='--', linewidth=2, label=f'Average from 1990-2023 ({y_value:.2f}%)')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Average Missing Data Percentage')\n",
    "plt.title('Average Missing Data Percentage by Year')\n",
    "plt.xticks(decade_years, rotation=45)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Period Selection Conclusion\n",
    "\n",
    "**Selected Period: 1990-2023**\n",
    "The analysis shows that the period from 1990 to 2023 provides an optimal balance between:\n",
    "- **Data availability**: Average missing data percentage is 17.39%\n",
    "- **Time span**: 34 years of data for robust analysis\n",
    "- **Data quality**: Below the 20% threshold for acceptable missing data.\n",
    "\n",
    "This period will be used for all subsequent analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"country-analysis\"></a>\n",
    "## 4. Country Selection Analysis\n",
    "### Preparing Data for Country-Level Analysis\n",
    "Extract data for the selected time period and prepare for country-level missing data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data for 1990-2023 period\n",
    "list_1990=[]\n",
    "\n",
    "for i, df in enumerate(df_list,1):\n",
    "    # Extract country column (first column, excluding header rows)\n",
    "    df_i=df.iloc[1:-2,0] # Skip header and footer rows\n",
    "    # Extract data for years 1990-2023\n",
    "    df_col= df.iloc[1:-2,:].loc[:,1990:2023]\n",
    "    # Combine country names with year data\n",
    "    df_1990=pd.concat([df_i,df_col],axis=1,ignore_index=False)\n",
    "    list_1990.append(df_1990)\n",
    "    print(f\"   Shape: {df_1990.shape}\")\n",
    "    print(f\"   Sample countries: {df_1990.iloc[:3, 0].tolist()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Missing Data by Country\n",
    "Find the missing percentage for each country on each metric. **Note** that NaN translates as the country is not included in the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate missing data percentages by country\n",
    "miss_1990=[]\n",
    "\n",
    "for df_m in list_1990:\n",
    "    # Transpose to have countries as columns\n",
    "    df_m=df_m.T\n",
    "    # Set first row as column names (country names)\n",
    "    df_m.columns = df_m.iloc[0]\n",
    "    # Calculate missing data percentage for each country\n",
    "    miss_1990.append(df_m.isnull().mean()*100)\n",
    "\n",
    "df_miss_country=pd.concat(miss_1990,axis=1,keys=Col_names)\n",
    "# Sample of missing data by country\n",
    "print(df_miss_country.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Country Data Quality Distribution\n",
    "Create visualizations to understand the distribution of data quality across countries. We fill the nan values with 100 to map as 100% missing data. Make a histogram to visualise the missing data per country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average missing data percentage across all metrics for each country\n",
    "avg_miss = df_miss_country.fillna(100).mean(axis=1)\n",
    "\n",
    "# Overall distribution histogram\n",
    "plt.hist(avg_miss)\n",
    "plt.ylabel('Count')\n",
    "print('Total number of Countries: ' + str(avg_miss.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are a lot of countries above 50 % but a sizeble number still remane under 40%. Let's Focus on countries with less than 40% missing data for detailed analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze countries with <40% missing data\n",
    "miss_40 = avg_miss[avg_miss < 40]\n",
    "print('Number of Counries with average missing persentage below 40: '+ str(miss_40.count()))\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "\n",
    "miss_40.plot(kind='hist', color='red', alpha=0.7)\n",
    "plt.title('Countries with Average Percentage Below 40%')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Optional)** Find the countries that have less than 40% average missing data in all metrics. This criteria results in a more restricted selection than we choose later. It's commented out so we can select it later if interested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "filtered_list=[]\n",
    "for i,df in enumerate(list_1990):\n",
    "    name=Col_names[i]\n",
    "    df = df.set_index(df.columns[0])\n",
    "    common_indexes = df.index.intersection(miss_40.index)\n",
    "    filtered_df = df.loc[common_indexes]\n",
    "    filtered_df.columns.name = str(name)\n",
    "    filtered_list.append(filtered_df)\n",
    "    print(filtered_df.head)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Country Selection: Balanced Approach\n",
    "\n",
    "\n",
    "**Goal**: Find coutries that have more than 70% in all metrics.\n",
    "\n",
    "**Selection Criteria**: Countries with ≤30% missing data in ANY metric.\n",
    "This approach:\n",
    "- Removes countries with >30% missing data in any single metric\n",
    "- Balances data quality with country coverage\n",
    "\n",
    "**Note**: We don't fill NaN with 100, because NaN translates as the country not exicting in the table so filling with 100 will result in removing it entierly from the list and missing infromation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter countries: remove those with >30% missing data in ANY metric\n",
    "df_miss_30 = df_miss_country[~(df_miss_country >30).any(axis=1)]\n",
    "\n",
    "# Countries meeting criteria\n",
    "print('Number of selected countries: ' + str(df_miss_30.shape[0]))\n",
    "\n",
    "filtered_list=[]\n",
    "for i,df in enumerate(list_1990):\n",
    "    name=Col_names[i]\n",
    "    df = df.set_index(df.columns[0])\n",
    "    common_indexes = df.index.intersection(df_miss_30.index)\n",
    "    filtered_df = df.loc[common_indexes]\n",
    "    filtered_df.columns.name = str(name)\n",
    "    filtered_list.append(filtered_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Dataset Country Coverage Analysis\n",
    "Identify countries that exicts in ALL datasets after filtering. If we have chosen different criteria their might be more countries across all tables that might allow for a group, so we can make deeper analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with countries from first dataset\n",
    "common_ind = set(filtered_list[0].index)\n",
    "\n",
    "# Find intersection with all other datasets\n",
    "for df in filtered_list[1:]:\n",
    "    common_ind.intersection_update(df.index)\n",
    "\n",
    "print(\"Common indexes:\", common_ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"data-imputation\"></a>\n",
    "## 5. Data Imputation and Final Cleaning\n",
    "### Missing Data Imputation Strategy\n",
    "**Approach**: Linear interpolation with constraints\n",
    "- **Method**: Linear interpolation for temporal data\n",
    "- **Direction**: Both forward and backward (leading and trailing NaNs)\n",
    "- **Limit**: Maximum 5 consecutive missing values\n",
    "- **Final step**: Drop any remaining rows with missing values\n",
    "\n",
    "This ensures we maintain data quality while filling reasonable gaps in time series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply interpolation to fill missing values\n",
    "filled_list=[]\n",
    "for i,df in enumerate(filtered_list):\n",
    "    # Count missing values before imputation\n",
    "    missing_counts = df.isnull().sum().sum()\n",
    "    print(f'Table {i+1}: Missing Before  '+str(missing_counts))\n",
    "    df_fill=df.interpolate(\n",
    "        method='linear', # Linear interpolation\n",
    "        limit_direction='both', # Fill both leading and trailing NaNs\n",
    "        limit=5, # Maximum 5 consecutive missing values\\n\n",
    "        axis=1 # Interpolate along columns (years)\n",
    "        )  \n",
    "    # Count missing values after imputation\n",
    "    missing_counts = df_fill.isnull().sum().sum()\n",
    "    print(f'Table {i+1}: Missing After   '+str(missing_counts)+'\\n')\n",
    "    # Drop rows that still have missing values\n",
    "    df_fill.dropna(inplace=True)\n",
    "    filled_list.append(df_fill)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"bis-integration\"></a>\n",
    "## 6. BIS Data Integration\n",
    "### Bank for International Settlements (BIS) Data Processing\n",
    "The BIS data has a different structure and requires separate processing:\n",
    "- **Metrics**: Debt service ratios for households and corporations\n",
    "- **Period**: 1999-2023 (different from main dataset) and orginally quarterly. \n",
    "- **Sheet**: Data is in sheet 2 of the Excel files after averaging annually the original quarterly data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bis_list=[]\n",
    "for item in ignore_list:\n",
    "    # Extract filename for labeling\n",
    "    df_name=item.split('\\\\')[-1]\n",
    "    df_name=df_name.replace('.xlsx', '') \n",
    "    df_bis = pd.read_excel(item, engine=\"openpyxl\",sheet_name=2)\n",
    "    # Set country names as index\n",
    "    df_bis = df_bis.set_index(df_bis.columns[0])\n",
    "    # Rename year columns from string to integer\n",
    "    df_bis.columns.name = str(df_name)\n",
    "    for i in range(1999,2024):\n",
    "        df_bis.rename(columns={f'{i}':i},inplace=True)\n",
    "\n",
    "    # Remove 2024 column\n",
    "    df_bis.drop(columns=['2024'],inplace=True)\n",
    "    # Sort columns by year\n",
    "    df_bis.sort_index(axis=1, inplace=True)\n",
    "    df_bis.index.name = None\n",
    "    bis_list.append(df_bis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining All Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list= filled_list + bis_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"data-format\"></a>\n",
    "## 7. Data Format Transformation\n",
    "### Converting to Long Format\n",
    "Transform the wide-format data (countries × years) into long format for easier analysis and storage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create metric names list (combining main and BIS datasets)\n",
    "names =Col_names+ ['Debt service ratios - Households (BIS)', 'Debt service ratios - Nonfinancial corporate (BIS)']  # Replace with your actual names\n",
    "\n",
    "# Combine all datasets with metric identifiers\n",
    "# Create combined DataFrame with hierarchical index\n",
    "combined_df = pd.concat(data_list, keys=names, names=['Metric', 'Country'])\n",
    "\n",
    "# Convert to long format\n",
    "long_df = combined_df.reset_index().melt(id_vars=['Metric', 'Country'], \n",
    "                           var_name='Year', \n",
    "                           value_name='Value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating User-Friendly Variable Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define mapping from long names to short, analysis-friendly names\n",
    "column_mapping = {\n",
    "    'GDP, current prices (Billions of U.S. dollars)': 'GDP_USD_bn',\n",
    "    'General Government Debt (Percent of GDP)': 'Govt_Debt_GDP',\n",
    "    'Household debt, all instruments (Percent of GDP)': 'Household_Debt_GDP',\n",
    "    'Inflation rate, average consumer prices (Annual percent change)': 'Inflation',\n",
    "    'Interest paid on public debt, percent of GDP (% of GDP)': 'Interest_Paid_GDP',\n",
    "    'Nonfinancial Public Sector Debt (Percent of GDP)': 'Public_Sector_Debt_GDP',\n",
    "    'Nonfinancial corporate debt, all instruments (Percent of GDP)': 'Corporate_Debt_GDP',\n",
    "    'Population (Millions of people)': 'Population_mn',\n",
    "    'Unemployment rate (Percent)': 'Unemployment',\n",
    "    'Debt service ratios - Households (BIS)': 'Debt_Service_Households',\n",
    "    'Debt service ratios - Nonfinancial corporate (BIS)': 'Debt_Service_Corporates'\n",
    "}\n",
    "\n",
    "long_df['Metric'] = long_df['Metric'].replace(column_mapping)\n",
    "\n",
    "long_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Summary and Export\n",
    "Create a final summary of the processed dataset and export it for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"   Total observations: {len(long_df)}\")\n",
    "print(f\"   Unique countries: {long_df['Country'].nunique()}\")\n",
    "print(f\"   Unique metrics: {long_df['Metric'].nunique()}\")\n",
    "print(f\"   Time period: {long_df['Year'].min()} - {long_df['Year'].max()}\")      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_df.to_csv('Processed_Data.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
